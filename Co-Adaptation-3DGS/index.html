<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <link rel="icon" href="logo.svg">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="style.css" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Co-Adaptation of 3DGS: Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting</title>
    <meta name="description" content="WildGaussians paper. Official web with qualitative comparisons, links to the source code, and additional materials.">
    <meta name="keywords" content="wildgaussians,3dgs,nerf,official,code" />
    <meta name="author" content="Jonas Kulhanek" />
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/@tabler/icons-webfont@3.7.0/dist/tabler-icons.min.css" rel="stylesheet">
  </head>
  <body>
    <header>
      <h1>
        <span class="title-main"><img src="logo.svg" /><span>Quantifying and Alleviating Co-Adaptation in <br> Sparse-View 3D Gaussian Splatting</span></span>
<!--         <span class="title-small">Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting</span> -->
      </h1>
      <div class="conference" style="font-style: normal;">Arxiv 2025</div>
    </header>
    <div class="authors">
      <div class="author">
        <span class="author-name">
          <a href="#">Kangjie Chen</a>
        </span>
        <span class="author-affiliation">Tsinghua University</span>
      </div>
      
      <div class="author">
        <span class="author-name">
          <a href="#">Yingji Zhong</a>
        </span>
        <span class="author-affiliation">HKUST</span>
      </div>
      
      <div class="author">
        <span class="author-name">
          <a href="#">Zhihao Li</a>
        </span>
        <span class="author-affiliation">Huawei Noah’s Ark Lab</span>
      </div>
      
      <div class="author">
        <span class="author-name">
          <a href="#">Jiaqi Lin</a>
        </span>
        <span class="author-affiliation">Tsinghua University</span>
      </div>

      <br>
      
      <div class="author">
        <span class="author-name">
          <a href="#">Youyu Chen</a>
        </span>
        <span class="author-affiliation">Harbin Institute of Technology</span>
      </div>
      
      <div class="author">
        <span class="author-name">
          <a href="#">Minghan Qin</a>
        </span>
        <span class="author-affiliation">Tsinghua University</span>
      </div>
      
      <div class="author">
        <span class="author-name">
          <a href="#">Haoqian Wang</a>
        </span>
        <span class="author-affiliation">Tsinghua University</span>
      </div>

    </div>
    <div class="links">
      <a class="button" href="https://arxiv.org/pdf/2407.08447"><i class="ti ti-file-type-pdf"></i> Paper</a>
      <a class="button" href="https://github.com/jkulhanek/wild-gaussians/"><i class="ti ti-brand-github-filled"></i> Code</a>
      <a class="button" href="https://youtu.be/Ri-er40QUoU"><i class="ti ti-slideshow"></i> Video</a>
    </div>
    <style>
      .video.teaser-video::before {
        padding-bottom: 50%;
      }
    </style>
    <p class="justify" style="font-size: 1rem;margin: 0 0 0.4rem 0; text-align-last: center">
    <strong>TL;DR:</strong> This paper introduces the concept of co-adaptation in 3D Gaussian Splatting, analyzes its impact on rendering artifacts, and proposes strategies (Dropout Regularization & Opacity Noise Injection) to reduce it.
    </p>

    <section class="abstract">
      <h2>Abstract</h2>
      <p>
3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel view synthesis under dense-view settings. However, in sparse-view scenarios, despite the realistic renderings in training views, 3DGS occasionally manifests appearance artifacts in novel views. This paper investigates the appearance artifacts in sparse-view 3DGS and uncovers a core limitation of current approaches: the optimized Gaussians are overly-entangled with one another to aggressively fit the training views, which leads to a neglect of the real appearance distribution of the underlying scene and results in appearance artifacts in novel views. The analysis is based on a proposed metric, termed Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians, i.e., co-adaptation, by computing the pixel-wise variance across multiple renderings of the same viewpoint, with different random subsets of Gaussians. The analysis reveals that the degree of co- adaptation is naturally alleviated as the number of training views increases. Based on the analysis, we propose two lightweight strategies to explicitly mitigate the co- adaptation in sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise injection to the opacity. Both strategies are designed to be plug-and-play, and their effectiveness is validated across various methods and benchmarks. We hope that our insights into the co-adaptation effect will inspire the community to achieve a more comprehensive understanding of sparse-view 3DGS.
      </p>
      <figure style="margin: 0">
        <img src="teaser.png" alt="Co-Adaptation of 3DGS overview" style="width: 100%; margin: 1em auto 0.3em auto; display: block;" />
        <figcaption>
        <strong>Visualization of 3DGS behaviors under different levels of co-adaptation.</strong> Thin gray arrows indicate training views, bold arrows indicate a novel view. Green arrow denotes correct color prediction, while red indicates color errors. (a) Simulates a 3DGS model trained with dense views, where Gaussian ellipsoids contribute evenly to pixel color across views, resulting in accurate rendering from the novel view. (b)(c)(d) Simulate various cases of 3DGS trained under sparse-view settings. (b) and (c) show that co-adaptation in the training views — where Gaussians contribute unequally to pixel colors — results in thin and thick artifacts under novel views. (d) shows a highly co-adapted case where multiple Gaussians with distinct colors collectively overfit a single grayscale pixel in the training view, resulting in severe wrong color artifacts under the novel view.
        </figcaption>
      </figure>
    </section>
    <section>
      <h2>Quantifying Co-Adaptation</h2>
      <p>
        To quantitatively analyze co-adaptation in 3D Gaussian Splatting, we define a <strong>Co-Adaptation Score (CA)</strong> for each target viewpoint. The key idea is that if a set of Gaussians are overly dependent on each other, then randomly removing part of them during rendering will lead to unstable outputs. Specifically, we randomly drop 50% of the Gaussians and render the target view using only the remaining ones. We repeat this process multiple times and measure the variance across the rendered results.
      </p>
      <figure style="margin: 0">
        <img src="CA_calculate.png" alt="Quantifying Co-Adaptation of 3DGS" style="width: 100%; margin: 1em auto 0.3em auto; display: block;" />
        <figcaption>
        <strong>Illustration of Co-Adaptation Score (CA) Computation.</strong> Higher CA scores indicate more inconsistent renderings, suggesting stronger co-adaptation effects. Lower CA scores reflect more stable and generalizable representations.
        </figcaption>
      </figure>
      
<!--       <figure>
        <div class="video-wrapper">
          <video class="video-compare" style="aspect-ratio: 1920/1080" loop muted>
            <source src="./assets/kplanes-wg-trevi.webm" type="video/webm">
            <source src="./assets/kplanes-wg-trevi.mp4" type="video/mp4">
          </video>
          <span class="video-label video-label-with-fps">
            <span>K-Planes<div class="video-fps">FPS: 0.15</div></span>
            <span>WildGaussians<div class="video-fps">FPS: 6.5</div></span>
          </span>
        </div>
      </figure>
      <figure>
        <div class="video-wrapper">
          <video class="video-compare" style="aspect-ratio: 1920/1080" loop muted>
            <source src="./assets/nerfwr-wg-sacrecoeur.webm" type="video/webm">
            <source src="./assets/nerfwr-wg-sacrecoeur.mp4" type="video/mp4">
          </video>
          <span class="video-label video-label-with-fps">
            <span>NeRF-W (reimpl.)<div class="video-fps">FPS: 0.004</div></span>
            <span>WildGaussians<div class="video-fps">FPS: 10.6</div></span>
          </span>
        </div>
      </figure> -->
    </section>
<!--     <section>
      <h2>Appearance interpolation</h2>
      <p class="justify">
      We show that our approach is able to smoothly interpolate between different appearances of the same scene.
      </p>
      <video class="video" style="aspect-ratio: 1920/1080;" loop muted id="appearance-interpolation-video">
        <source src="./assets/wg-trevi-app-interpolation.webm" type="video/webm">
        <source src="./assets/wg-trevi-app-interpolation.mp4" type="video/mp4">
      </video>
      <div style="margin: 0.4rem 10% 6px 10%; margin-left: calc(10% - 13px); margin-right: calc(10% - 10px);">
        <input type="range" min="1" max="100" value="0" step="0.0001" class="slider" data-control-slider-images="slider-bar-1" data-control-video="appearance-interpolation-video" />
      </div>
      <div style="display:flex; justify-content: space-between;" id="slider-bar-1">
        <div class="slider-image" style="--active-weight:100%"><img src="./assets/appimg0.jpg" /></div>
        <div class="slider-image"><img src="./assets/appimg1.jpg" /></div>
        <div class="slider-image"><img src="./assets/appimg2.jpg" /></div>
      </div>
    </section>
    <section>
      <h2>Removing occluders</h2>
      <p class="justify">
      When there are occluders in the scene, the Gaussian splatting will not be able to represent the scene correctly leading to excessive ammounts of <strong>floaters</strong>. Our approach can remove these occluders by using DINO-based uncertainty predictor.
      Note, we report FPS computed on NVIDIA 4090 at FullHD resolution (1920x1080).
      </p>
      <figure>
        <div class="video-wrapper">
          <video class="video-compare" style="aspect-ratio: 1920/1080" loop muted>
            <source src="./assets/3dgs-wg-patio-high.webm" type="video/webm">
            <source src="./assets/3dgs-wg-patio-high.mp4" type="video/mp4">
          </video>
          <span class="video-label video-label-with-fps">
            <span>3DGS<div class="video-fps">FPS: 18.8</div></span>
            <span>WildGaussians<div class="video-fps">FPS: 16.0</div></span>
          </span>
        </div>
      </figure>
      <figure>
        <div class="video-wrapper">
          <video class="video-compare" style="aspect-ratio: 1920/1080" loop muted>
            <source src="./assets/nerfonthego-wg-spot.webm" type="video/webm">
            <source src="./assets/nerfonthego-wg-spot.mp4" type="video/mp4">
          </video>
          <span class="video-label video-label-with-fps">
            <span>NeRF On-the-go<div class="video-fps">FPS: 0.05</div></span>
            <span>WildGaussians<div class="video-fps">FPS: 12.6</div></span>
          </span>
        </div>
      </figure>
      <figure>
        <div class="video-wrapper">
          <video class="video-compare" style="aspect-ratio: 1920/1080" loop muted>
            <source src="./assets/3dgs-wg-corner.webm" type="video/webm">
            <source src="./assets/3dgs-wg-corner.mp4" type="video/mp4">
          </video>
          <span class="video-label video-label-with-fps">
            <span>3DGS<div class="video-fps">FPS: 7.5</div></span>
            <span>WildGaussians<div class="video-fps">FPS: 12.6</div></span>
          </span>
        </div>
      </figure>
      <figure>
        <div class="video-wrapper">
          <video class="video-compare" style="aspect-ratio: 1920/1080" loop muted>
            <source src="./assets/nerfonthego-wg-patio.webm" type="video/webm">
            <source src="./assets/nerfonthego-wg-patio.mp4" type="video/mp4">
          </video>
          <span class="video-label video-label-with-fps">
            <span>NeRF On-the-go<div class="video-fps">FPS: 0.05</div></span>
            <span>WildGaussians<div class="video-fps">FPS: 8.6</div></span>
          </span>
        </div>
      </figure>
    </section>
    <section>
      <h2>Depth prediction</h2>
      <p class="justify">
      For reference, we show the depth prediction rendered by rasterizing the Gaussians' centers.
      </p>
      <figure>
        <div class="video-wrapper">
          <video class="video-compare" style="aspect-ratio: 1920/1080" loop muted>
            <source src="./assets/wg-trevi-depth.webm" type="video/webm">
            <source src="./assets/wg-trevi-depth.mp4" type="video/mp4">
          </video>
          <span class="video-label"><span>RGB</span><span>Depth</span></span>
        </div>
      </figure>
    </section> -->
    <section>
      <h2>Concurrent works</h2>
      <p class="justify">
      There are two another concurrent works that also use dropout to boost sparse-view 3DGS:
      <ul>
        <li><a href="https://arxiv.org/abs/2504.00773">DropGaussian: Structural Regularization for Sparse-view Gaussian Splatting</li></a>
        <li><a href="https://arxiv.org/abs/2504.09491">DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering</a></li>
      </ul>
      However, they attribute the effectiveness of dropout to empirical factors—such as reducing overfitting through fewer active splats (DropoutGS), or enhancing gradient flow to distant Gaussians (DropGaussian).
      </p>
    </section>
<!--     <section>
      <h2>Acknowledgements</h2>
      <p class="justify">
        We would like to thank Weining Ren for his help with the NeRF On-the-go dataset and code and Tobias Fischer and Xi Wang for fruitful discussions.
        This work was supported by the Czech Science Foundation (GAČR) EXPRO (grant no. 23-07973X)
        and by the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90254).
        The renderer is built on 3DGS, Mip-Splatting. Please follow the license of 3DGS and Mip-Splatting. 
        We thank all the authors for their great work and repos. Finally, we would also like to thank 
        Dor Verbin for the video comparison tool used in this website.
      </p>
    </section>
    <section class="citation">
      <h2>Citation</h2>
      <span>Please use the following citation:</span>
      <pre><code>@article{kulhanek2024wildgaussians,
  title={{W}ild{G}aussians: {3D} Gaussian Splatting in the Wild},
  author={Kulhanek, Jonas and Peng, Songyou and Kukelova, Zuzana and Pollefeys, Marc and Sattler, Torsten},
  journal={NeurIPS},
  year={2024}
}</code></pre>
    </section> -->
    <script src="./scripts.js"></script>
  </body>
</html>
